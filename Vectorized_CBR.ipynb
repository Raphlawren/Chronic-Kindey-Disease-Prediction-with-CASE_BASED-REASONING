{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "949c7941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from collections import Counter\n",
    "import math\n",
    "import copy\n",
    "\n",
    "\n",
    "data = pd.read_csv('Binary_data.csv')\n",
    "# data = pd.read_csv('/Users/macbookair/workspace/COM804/Approach_1/Multi_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b499e",
   "metadata": {},
   "source": [
    "### Binary Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b919723",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9aadce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(969, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b01683ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=31)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size= 0.5, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59cd31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "# X_val = X_val.reset_index(drop=True)\n",
    "# y_val = y_val.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb6bf809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  678\n",
      "Test size:  291\n"
     ]
    }
   ],
   "source": [
    "print(\"Training size: \", X_train.shape[0])\n",
    "print(\"Test size: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bbf220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(X, X_train):\n",
    "    \n",
    "    eps = 1e-12\n",
    "    X_min = X_train.min(axis=0)\n",
    "    X_max = X_train.max(axis=0)\n",
    "    diff  = np.maximum(X_max - X_min, eps)\n",
    "    X_norm = (X - X_min) / diff\n",
    "    \n",
    "    return X_norm\n",
    "\n",
    "# X_train_norm = normalize_features(X_train, X_train)\n",
    "# X_val_norm = normalize_features(X_val, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7a1d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = normalize_features(X_train, X_train)\n",
    "# X_val_norm = normalize_features(X_val, X_train)\n",
    "X_test_norm = normalize_features(X_test, X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8fabbc16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Systolic_Pressure</th>\n",
       "      <th>BMI</th>\n",
       "      <th>CKD_Cause</th>\n",
       "      <th>Hemoglobin</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Creatinine</th>\n",
       "      <th>eGFR</th>\n",
       "      <th>CKD_Stage</th>\n",
       "      <th>CKD_Risk</th>\n",
       "      <th>Dipstick_Proteinuria</th>\n",
       "      <th>Proteinuria</th>\n",
       "      <th>Occult_Blood_in_Urine</th>\n",
       "      <th>Protein_Creatinine_Ratio</th>\n",
       "      <th>UPCR_Severity</th>\n",
       "      <th>Hypertension</th>\n",
       "      <th>Previous_CVD</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>RAAS_Inhibitor</th>\n",
       "      <th>Calcium_Channel_Blocker</th>\n",
       "      <th>Diuretics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223684</td>\n",
       "      <td>0.237548</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.731092</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.091116</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009415</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.861111</td>\n",
       "      <td>0.355263</td>\n",
       "      <td>0.291188</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.521008</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.476082</td>\n",
       "      <td>0.077676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025273</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.915966</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.148064</td>\n",
       "      <td>0.516477</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018335</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.425287</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.487395</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.248292</td>\n",
       "      <td>0.317270</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009911</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.537815</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.179954</td>\n",
       "      <td>0.429138</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sex       Age  Systolic_Pressure       BMI  CKD_Cause  Hemoglobin  \\\n",
       "0  0.0  0.000000           0.223684  0.237548   0.666667    0.731092   \n",
       "1  1.0  0.861111           0.355263  0.291188   0.333333    0.521008   \n",
       "2  0.0  0.611111           0.368421  0.241379   0.333333    0.915966   \n",
       "3  0.0  0.750000           0.250000  0.425287   0.333333    0.487395   \n",
       "4  0.0  0.708333           0.467105  0.517241   0.333333    0.537815   \n",
       "\n",
       "    Albumin  Creatinine      eGFR  CKD_Stage  CKD_Risk  Dipstick_Proteinuria  \\\n",
       "0  0.864865    0.091116  1.000000   0.000000  0.090909                   0.4   \n",
       "1  0.837838    0.476082  0.077676   1.000000  1.000000                   0.0   \n",
       "2  0.702703    0.148064  0.516477   0.333333  0.363636                   0.2   \n",
       "3  0.648649    0.248292  0.317270   0.333333  0.363636                   0.0   \n",
       "4  0.648649    0.179954  0.429138   0.333333  0.272727                   0.0   \n",
       "\n",
       "   Proteinuria  Occult_Blood_in_Urine  Protein_Creatinine_Ratio  \\\n",
       "0          1.0                    0.0                  0.009415   \n",
       "1          0.0                    0.0                  0.025273   \n",
       "2          0.0                    0.0                  0.018335   \n",
       "3          0.0                    1.0                  0.009911   \n",
       "4          0.0                    1.0                  0.000991   \n",
       "\n",
       "   UPCR_Severity  Hypertension  Previous_CVD  Diabetes  RAAS_Inhibitor  \\\n",
       "0            0.5           0.0           0.0       0.0             0.0   \n",
       "1            1.0           1.0           0.0       0.0             1.0   \n",
       "2            0.5           1.0           0.0       0.0             1.0   \n",
       "3            0.5           1.0           0.0       0.0             1.0   \n",
       "4            0.0           1.0           0.0       0.0             1.0   \n",
       "\n",
       "   Calcium_Channel_Blocker  Diuretics  \n",
       "0                      0.0        0.0  \n",
       "1                      0.0        1.0  \n",
       "2                      1.0        0.0  \n",
       "3                      0.0        0.0  \n",
       "4                      1.0        0.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fb44dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['Age', 'Systolic_Pressure', 'BMI', 'Hemoglobin', 'Albumin', 'Creatinine', 'eGFR', 'Protein_Creatinine_Ratio']\n",
    "cat_cols = [c for c in X_train if c not in num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24c75830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 2\n",
      "Age: 66\n",
      "Systolic_Pressure: 107\n",
      "BMI: 169\n",
      "CKD_Cause: 4\n",
      "Hemoglobin: 107\n",
      "Albumin: 36\n",
      "Creatinine: 274\n",
      "eGFR: 626\n",
      "CKD_Stage: 4\n",
      "CKD_Risk: 12\n",
      "Dipstick_Proteinuria: 6\n",
      "Proteinuria: 2\n",
      "Occult_Blood_in_Urine: 2\n",
      "Protein_Creatinine_Ratio: 315\n",
      "UPCR_Severity: 3\n",
      "Hypertension: 2\n",
      "Previous_CVD: 2\n",
      "Diabetes: 2\n",
      "RAAS_Inhibitor: 2\n",
      "Calcium_Channel_Blocker: 2\n",
      "Diuretics: 2\n"
     ]
    }
   ],
   "source": [
    "### printt the number of unique values in each feature\n",
    "for i in X_train.columns:\n",
    "    print(f\"{i}: {X_train[i].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "538ee51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## seperating the numerical columns and the categorical columns\n",
    "cat_cols = [col for col in X_train.columns if X_train[col].nunique() <= 12]\n",
    "num_cols = [c for c in X_train if c not in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0c16982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the weights to 1\n",
    "# W = {}\n",
    "# for col in X_train.columns:\n",
    "#     W[col] = 1.0\n",
    "n = X_train_norm.shape[1]\n",
    "W = np.ones(n,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e4041f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### similarity search\n",
    "def local_sims(new_case, casebase, num_cols, cat_cols, eps=1e-12):\n",
    "    sims = {}\n",
    "    \n",
    "    for col in num_cols:\n",
    "            diff = np.abs(new_case[col] - casebase[col])\n",
    "            f_min = casebase[col].min()\n",
    "            f_max = casebase[col].max()\n",
    "            f_range = np.maximum(f_max - f_min, eps)\n",
    "            sims[col] = (1 - (diff / f_range)).clip(0, 1)\n",
    "        \n",
    "    for col in cat_cols:\n",
    "            sims[col] = (casebase[col] == new_case[col]).astype(float)\n",
    "    \n",
    "    df_sim = pd.DataFrame(sims, index=casebase.index)\n",
    "    ordered_cols = [c for c in casebase.columns if c in df_sim.columns]\n",
    "    \n",
    "    return df_sim[ordered_cols]\n",
    "\n",
    "### gloabl similarity search\n",
    "\n",
    "def global_sims(X, W, top_k):\n",
    "    W = np.asarray(W, dtype=float)\n",
    "    X = X.to_numpy(dtype=float)\n",
    "    \n",
    "    gs = (X * W).sum(axis=1) / (W.sum() + 1e-12) ## avoid dividing by zero\n",
    "    sorted_gs = np.argsort(gs)[::-1][:top_k]          ## returns the top 5 rows of the most similar cases\n",
    "    # best_idx = np.argmax(gs)\n",
    "    scores = gs[sorted_gs]\n",
    "    \n",
    "    return scores, sorted_gs, gs\n",
    "\n",
    "\n",
    "# def linear_fxn(Z, activation):\n",
    "    \n",
    "#     if activation == 'softmax':\n",
    "#         ez = np.exp(Z)\n",
    "#         sm = ez / np.sum(ez)\n",
    "    \n",
    "#     elif activation == 'sigmoid':\n",
    "\n",
    "#         sm = 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "#     return sm\n",
    "\n",
    "def linear_fxn(Z, activation):\n",
    "    Z = np.asarray(Z, dtype=float)\n",
    "\n",
    "    if activation == 'softmax':\n",
    "        Z = Z - np.max(Z)\n",
    "        ez = np.exp(Z)\n",
    "        return ez / (np.sum(ez) + 1e-12)\n",
    "\n",
    "    elif activation == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"activation must be 'softmax' or 'sigmoid'\")\n",
    "\n",
    "\n",
    "# retrieve the similar codes with the actual\n",
    "def retrieval(idx, casebase, y_train): \n",
    "    retrieved_x = casebase.iloc[idx].copy()\n",
    "    retrieved_y = y_train.iloc[idx]\n",
    "    # retrieved_x['CKD Progression'] = retrieved_y.to_numpy()\n",
    "    \n",
    "    return retrieved_x, retrieved_y\n",
    "\n",
    "## make prediction of the result\n",
    "# hard retrieval -----\n",
    "\"\"\"\n",
    "Majority\n",
    "\"\"\"\n",
    "def reuser(retrieved_y):\n",
    "    yhat = np.asarray(retrieved_y).astype(int)\n",
    "    \n",
    "    return int(np.bincount(yhat).argmax())\n",
    "\n",
    "#soft retrieval\n",
    "def reuse(retrieved_y, scores):\n",
    "    # y = retrieved_y.to_numpy()\n",
    "    y = np.asarray(retrieved_y)\n",
    "    \n",
    "    s = np.asarray(scores)\n",
    "    \n",
    "    w = linear_fxn(s, activation='softmax')\n",
    "    \n",
    "    p = np.dot(w, y) \n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "## predict for a sinle case\n",
    "def prediction(new_case, casebase, y_train,  num_cols, cat_cols, W, top_k = 5):\n",
    "    ls = local_sims(new_case, casebase, num_cols, cat_cols)\n",
    "    scores, idx, _ = global_sims(ls, W, top_k)\n",
    "    _, y_retrieved = retrieval(idx, casebase, y_train)\n",
    "    # yhat = y_retrieved.value_counts().idxmax()\n",
    "    # yhat = reuse(y_retrieved)\n",
    "    yhat = reuse(y_retrieved, scores)\n",
    "        \n",
    "    return yhat, idx, scores\n",
    "\n",
    "\n",
    "#predict for multiple cases\n",
    "def multi_prediction(new_case, casebase, y_train,  num_cols, cat_cols, W, top_k = 5):\n",
    "    preds = []\n",
    "    for _, case in new_case.iterrows():\n",
    "        yhat, _, _ = prediction(case, casebase, y_train,  num_cols, cat_cols, W, top_k)\n",
    "        preds.append(yhat)\n",
    "        \n",
    "    return np.asarray(preds)\n",
    "\n",
    "\n",
    "## Cost function\n",
    "\n",
    "# def log_loss(preds, y):\n",
    "#     cost = 0.0\n",
    "#     y = np.asarray(y)\n",
    "#     m =  y.shape[0]\n",
    "#     for i in range(m):    \n",
    "#         cost +=  - y[i] * np.log(preds[i]) - (1 - y[i]) * np.log(1 - preds[i])\n",
    "#     cost = cost / m\n",
    "    \n",
    "#     return cost\n",
    "\n",
    "\n",
    "def log_loss(preds, y, eps=1e-12):\n",
    "    preds = np.asarray(preds)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    preds = np.clip(preds, eps, 1-eps)\n",
    "    \n",
    "    return -np.mean(y * np.log(preds) + (1 - y) * np.log(1 - preds))\n",
    "\n",
    "    \n",
    "    \n",
    "## compute loss\n",
    "def compute_loss_cbr(X_val, y_val, W, X_train_norm, y_train, num_cols, cat_cols, top_k =5):\n",
    "    probs = multi_prediction(X_val, X_train_norm, y_train,  num_cols, cat_cols, W, top_k)\n",
    "    loss_cbr = log_loss(probs, y_val)\n",
    "    return loss_cbr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Evaluation \n",
    "def evaluation(pred, actual):\n",
    "        pred = np.asarray(pred)\n",
    "        actual = np.asarray(actual)\n",
    "        # pred_class = (pred >= 0.5).astype(int)\n",
    "        misclassified = (pred != actual).sum()\n",
    "        fraction_error = misclassified / len(actual)\n",
    "        \n",
    "        return misclassified, fraction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9415e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## called the trained weights\n",
    "W1 = np.load('model_weights.npy')\n",
    "W2 = np.load('2model_weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85dcac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for _, case in X_test_norm.iterrows():\n",
    "    ls = local_sims(case, X_train_norm, num_cols, cat_cols)\n",
    "    scores, idx, gs = global_sims(ls, W2, top_k=3)\n",
    "    _, retrieved_y = retrieval(idx, X_train_norm, y_train)\n",
    "    p = reuser(retrieved_y)\n",
    "    preds.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63c24449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(291, 22)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a7ffb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 0.15463917525773196)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "929614a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 0.14432989690721648)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd371f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 0.140893470790378)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation(preds, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d22647",
   "metadata": {},
   "source": [
    "### GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d9fd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite_diff_grad(loss_fn, W, eps=1e-4):\n",
    "    W = np.asarray(W)\n",
    "    \n",
    "    grad = np.zeros_like(W)    \n",
    "    \n",
    "    for j in range(len(W)):\n",
    "        Wp = W.copy()\n",
    "        Wm = W.copy()\n",
    "        Wp[j] += eps\n",
    "        Wm[j] -= eps\n",
    "        \n",
    "        grad[j] = (loss_fn(Wp) - loss_fn(Wm)) / (2 * eps)\n",
    "        \n",
    "    return grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a9a95df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_val, y_val, W, alpha, num_iters, X_train_norm, y_train, num_cols, cat_cols, top_k=5, eps=1e-4):\n",
    "    \n",
    "    J_history = []\n",
    "    \n",
    "    w = copy.deepcopy(W)\n",
    "    def loss_fn(W_try):\n",
    "        return compute_loss_cbr(X_val,y_val, W_try, X_train_norm,y_train, num_cols, cat_cols, top_k=top_k)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        ## calculate \n",
    "        cost = loss_fn(w)\n",
    "        \n",
    "        grad = finite_diff_grad(loss_fn, w, eps=eps)\n",
    "        \n",
    "        w = w - alpha * grad\n",
    "        \n",
    "        \n",
    "        # if i < 100000:\n",
    "        #     J_history.append(compute_gradient_cbr(X, y_val, w, X_train_norm, y_train, num_cols, cat_cols, top_k=5))\n",
    "        \n",
    "        J_history.append(cost)\n",
    "            \n",
    "        if i% math.ceil(num_iters / 100) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}      \")\n",
    "            \n",
    "    return w, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2b09234b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 0.2579141458565357      \n",
      "Iteration    2: Cost 0.25791404816704655      \n",
      "Iteration    4: Cost 0.25791395047849297      \n",
      "Iteration    6: Cost 0.2579138527908748      \n",
      "Iteration    8: Cost 0.2579137551041922      \n",
      "Iteration   10: Cost 0.25791365741844485      \n",
      "Iteration   12: Cost 0.25791355973363295      \n",
      "Iteration   14: Cost 0.2579134620497563      \n",
      "Iteration   16: Cost 0.2579133643668149      \n",
      "Iteration   18: Cost 0.25791326668480863      \n",
      "Iteration   20: Cost 0.25791316900373756      \n",
      "Iteration   22: Cost 0.2579130713236016      \n",
      "Iteration   24: Cost 0.25791297364440063      \n",
      "Iteration   26: Cost 0.25791287596613477      \n",
      "Iteration   28: Cost 0.2579127782888037      \n",
      "Iteration   30: Cost 0.2579126806124077      \n",
      "Iteration   32: Cost 0.25791258293694647      \n",
      "Iteration   34: Cost 0.25791248526242005      \n",
      "Iteration   36: Cost 0.2579123875888284      \n",
      "Iteration   38: Cost 0.25791228991617143      \n",
      "Iteration   40: Cost 0.2579121922444491      \n",
      "Iteration   42: Cost 0.25791209457366143      \n",
      "Iteration   44: Cost 0.2579119969038083      \n",
      "Iteration   46: Cost 0.2579118992348896      \n",
      "Iteration   48: Cost 0.25791180156690546      \n",
      "Iteration   50: Cost 0.2579117038998557      \n",
      "Iteration   52: Cost 0.2579116062337403      \n",
      "Iteration   54: Cost 0.25791150856855927      \n",
      "Iteration   56: Cost 0.25791141090431235      \n",
      "Iteration   58: Cost 0.2579113132409998      \n",
      "Iteration   60: Cost 0.2579112155786214      \n",
      "Iteration   62: Cost 0.257911117917177      \n",
      "Iteration   64: Cost 0.2579110202566668      \n",
      "Iteration   66: Cost 0.2579109225970906      \n",
      "Iteration   68: Cost 0.2579108249384483      \n",
      "Iteration   70: Cost 0.25791072728073994      \n",
      "Iteration   72: Cost 0.2579106296239655      \n",
      "Iteration   74: Cost 0.25791053196812486      \n",
      "Iteration   76: Cost 0.25791043431321803      \n",
      "Iteration   78: Cost 0.25791033665924484      \n",
      "Iteration   80: Cost 0.2579102390062054      \n",
      "Iteration   82: Cost 0.25791014135409956      \n",
      "Iteration   84: Cost 0.25791004370292736      \n",
      "Iteration   86: Cost 0.2579099460526886      \n",
      "Iteration   88: Cost 0.2579098484033834      \n",
      "Iteration   90: Cost 0.25790975075501155      \n",
      "Iteration   92: Cost 0.2579096531075731      \n",
      "Iteration   94: Cost 0.25790955546106803      \n",
      "Iteration   96: Cost 0.25790945781549623      \n",
      "Iteration   98: Cost 0.2579093601708576      \n",
      "Iteration  100: Cost 0.25790926252715224      \n",
      "Iteration  102: Cost 0.25790916488438      \n",
      "Iteration  104: Cost 0.2579090672425408      \n",
      "Iteration  106: Cost 0.25790896960163473      \n",
      "Iteration  108: Cost 0.2579088719616616      \n",
      "Iteration  110: Cost 0.2579087743226214      \n",
      "Iteration  112: Cost 0.25790867668451406      \n",
      "Iteration  114: Cost 0.25790857904733966      \n",
      "Iteration  116: Cost 0.257908481411098      \n",
      "Iteration  118: Cost 0.2579083837757891      \n",
      "Iteration  120: Cost 0.257908286141413      \n",
      "Iteration  122: Cost 0.2579081885079694      \n",
      "Iteration  124: Cost 0.25790809087545846      \n",
      "Iteration  126: Cost 0.25790799324388003      \n",
      "Iteration  128: Cost 0.25790789561323413      \n",
      "Iteration  130: Cost 0.2579077979835207      \n",
      "Iteration  132: Cost 0.25790770035473964      \n",
      "Iteration  134: Cost 0.257907602726891      \n",
      "Iteration  136: Cost 0.2579075050999746      \n",
      "Iteration  138: Cost 0.25790740747399055      \n",
      "Iteration  140: Cost 0.2579073098489386      \n",
      "Iteration  142: Cost 0.2579072122248189      \n",
      "Iteration  144: Cost 0.2579071146016312      \n",
      "Iteration  146: Cost 0.2579070169793757      \n",
      "Iteration  148: Cost 0.2579069193580521      \n",
      "Iteration  150: Cost 0.25790682173766055      \n",
      "Iteration  152: Cost 0.25790672411820087      \n",
      "Iteration  154: Cost 0.2579066264996731      \n",
      "Iteration  156: Cost 0.2579065288820771      \n",
      "Iteration  158: Cost 0.25790643126541285      \n",
      "Iteration  160: Cost 0.2579063336496804      \n",
      "Iteration  162: Cost 0.25790623603487955      \n",
      "Iteration  164: Cost 0.2579061384210104      \n",
      "Iteration  166: Cost 0.2579060408080727      \n",
      "Iteration  168: Cost 0.2579059431960667      \n",
      "Iteration  170: Cost 0.257905845584992      \n",
      "Iteration  172: Cost 0.25790574797484883      \n",
      "Iteration  174: Cost 0.25790565036563706      \n",
      "Iteration  176: Cost 0.25790555275735655      \n",
      "Iteration  178: Cost 0.25790545515000735      \n",
      "Iteration  180: Cost 0.25790535754358934      \n",
      "Iteration  182: Cost 0.2579052599381026      \n",
      "Iteration  184: Cost 0.25790516233354693      \n",
      "Iteration  186: Cost 0.25790506472992236      \n",
      "Iteration  188: Cost 0.2579049671272289      \n",
      "Iteration  190: Cost 0.2579048695254663      \n",
      "Iteration  192: Cost 0.25790477192463473      \n",
      "Iteration  194: Cost 0.257904674324734      \n",
      "Iteration  196: Cost 0.2579045767257641      \n",
      "Iteration  198: Cost 0.2579044791277251      \n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "iters = 200\n",
    "\n",
    "w_out, histo = gradient_descent(X_val_norm, y_val, nweights, alpha, iters, X_train_norm, y_train, num_cols, cat_cols, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2bf6bfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights saved to model_weights.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example weight parameter (replace with your actual weight array)\n",
    "weights = np.array(w_out)\n",
    "\n",
    "# Save the weights to a file named 'model_weights.npy'\n",
    "np.save('2model_weights.npy', weights) \n",
    "\n",
    "print(\"Weights saved to model_weights.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd403df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96151141, 1.00244137, 1.00123219, 1.00157237, 1.05102703,\n",
       "       1.00234004, 1.00277769, 1.40610332, 0.99782801, 0.99544698,\n",
       "       0.99658982, 1.44311706, 0.99508233, 1.00142946, 0.99924447,\n",
       "       0.99507403, 0.95500146, 0.77109575, 0.99644874, 0.39405158,\n",
       "       1.96658946, 0.0803751 ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6802503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
